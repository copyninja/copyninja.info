Notes: Experimenting with ZRAM and Memory Over commit
#####################################################

:date: 2023-06-16 14:55
:slug: zram_memory_overcommit
:tags: zram, vm, overcommit, debian
:author: copyninja
:summary: Notes on experiments with ZRAM and vm.overcommit_memory

ZRAM module in Linux kernel creates memory backed block device which keeps its
content in compressed format. User can pick one of the compression algorithm
from lz4,zstd or lzo. Compression ratio of these algorithms are *zstd > lzo >
lz4* and speed is *lz4 > zstd > lzo*.

I was intrested in using ZRAM as swap to the system. There are 2 utilities which
can help in getting this done, *zram-tools* and *systemd-zram-generator*. Since
Debian Bullseye does not have *systemd-zram-generator*, only option for Bullseye
user is *zram-tools*. Of course people can use *systemd-zram-generator* by self
compiling or via *cargo* but I was working in restricted environment and prefer
using tools which is available in the distribution repository.

Installation was stragiht forward.

.. code-block:: shell

    apt-get install zram-tools

Configuration is simple shellscript file which is sourced by */usr/bin/zramswap*
script. Below is the configuration I used.

.. code-block:: shell

    # Compression algorithm selection
    # speed: lz4 > zstd > lzo
    # compression: zstd > lzo > lz4
    # This is not inclusive of all that is available in latest kernels
    # See /sys/block/zram0/comp_algorithm (when zram module is loaded) to see
    # what is currently set and available for your kernel[1]
    # [1]  https://github.com/torvalds/linux/blob/master/Documentation/blockdev/zram.txt#L86
    ALGO=zstd

    # Specifies the amount of RAM that should be used for zram
    # based on a percentage the total amount of available memory
    # This takes precedence and overrides SIZE below
    PERCENT=30

    # Specifies a static amount of RAM that should be used for
    # the ZRAM devices, this is in MiB
    #SIZE=256000

    # Specifies the priority for the swap devices, see swapon(2)
    # for more details. Higher number = higher priority
    # This should probably be higher than hdd/ssd swaps.
    #PRIORITY=100

I used *zstd* as it provides best compression and reserved *30%* of memory as size
of zram device. Post the modification we need to restart *zramswap.service* to
make the swap active.

.. code-block:: shell

    systemctl restart zramswap.service

The same above can be done using *systemd-zram-generator* package from Debian
Bookworm. *zram-tools* is still available in the Debian Bookworm but this gives
more tighter integration into systemd ecosystem. Following is the above
configuration translated to *systemd-zram-generator* and location is
*/etc/systemd/zram-generator.conf*.

.. code-block:: conf

    # This config file enables a /dev/zram0 swap device with the following
    # properties:
    # * size: 50% of available RAM or 4GiB, whichever is less
    # * compression-algorithm: kernel default
    #
    # This device's properties can be modified by adding options under the
    # `[zram0]` section, or disabled by removing the section header.
    # Additional zram devices can be created by appending new `[zramX]`
    # sections and setting the appropriate options for each device.
    #
    # See /usr/share/doc/systemd-zram-generator/zram-generator.conf.example
    # and/or zram-generator.conf(5) for a list of available options.
    [zram0]
    zram-size = ceil(ram * 33.3/100)
    compression-algorithm = zstd
    swap-priority = 100
    fs-type = swap

Post changing the above we need to reload systemd and start
*systemd-zram-setup@zram0.service*.

.. code-block:: shell

   systemctl daemon-reload
   systemctl start systemd-zram-setup@zram0.service

 *systemd-zram-generator* creates the zram device by loading kernel module and
 then creates *systemd.swap* unit for mounting zram device as swap. In this case
 swap file is called *zram0.swap* and here is its content.

 .. code-block:: ini

     # /lib/systemd/system/systemd-zram-setup@.service
     # SPDX-License-Identifier: MIT
     # This file is part of the zram-generator project
     # https://github.com/systemd/zram-generator

     [Unit]
     Description=Create swap on /dev/%i
     Documentation=man:zram-generator(8) man:zram-generator.conf(5)
     After=dev-%i.device
     DefaultDependencies=false

     [Service]
     Type=oneshot
     RemainAfterExit=yes
     ExecStart=/lib/systemd/system-generators/zram-generator --setup-device '%i'
     ExecStop=/lib/systemd/system-generators/zram-generator --reset-device '%i'

     # /run/systemd/generator/systemd-zram-setup@zram0.service.d/bindings.conf
     # Automatically generated by /usr/lib/systemd/system-generators/zram-generator

     [Unit]
     BindsTo=dev-%i.swap

Note that in this case I'm creating swap device but you can provide any file
system as *fs-type* and same will be created on the device and you can also
provide *mount-point* in which case *systemd.mount* file will be created to
mount the new device to the specified mount point.

When I tested the swap with some synthetic memory pressure created using
*stress-ng* I could reach compression ratio of approx *40%*. (In range of 36-40
to be exact). The status of zram device and compression details can be got by
using *zramctl* which is part of *util-linux* package. We can also use
*zramswap* which is the utility provided *zram-tools* which will give us same
output.

Memory Overcommit
=================

Another use case I wanted was the ability to launch applications more than the
amount permitted by the total system memory. If you ask why? well use case is
there are applications which due large amount of malloc but never uses all the
memory. Even if memory used not all applications will use at same time.

By default Linux kernel attempts to estimate the amount of free memory left on the
system when user space asks for more memory (*vm.overcommit_memory=0*). To try
this we can run stress-ng and request memory more than available on system.

.. code-block:: shell

    ┌─(~)─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────(vasudeva.sk@chamunda:pts/8)─┐
    └─(17:07:43)──> free -tg                                                                                                                                                                                         ──(Mon,Jun19)─┘
                   total        used        free      shared  buff/cache   available
    Mem:              31          12          11           3          11          18
    Swap:             10           2           8
    Total:            41          14          19
    ┌─(~)─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────(vasudeva.sk@chamunda:pts/8)─┐
    └─(17:07:46)──> sudo stress-ng --vm=1 --vm-bytes=50G -t 120                                                                                                                                                      ──(Mon,Jun19)─┘
    stress-ng: info:  [1496310] setting to a 120 second (2 mins, 0.00 secs) run per stressor
    stress-ng: info:  [1496310] dispatching hogs: 1 vm
    stress-ng: info:  [1496312] vm: gave up trying to mmap, no available memory, skipping stressor
    stress-ng: warn:  [1496310] vm: [1496311] aborted early, out of system resources
    stress-ng: info:  [1496310] vm:
    stress-ng: warn:  [1496310]         14 System Management Interrupts
    stress-ng: info:  [1496310] passed: 0
    stress-ng: info:  [1496310] failed: 0
    stress-ng: info:  [1496310] skipped: 1: vm (1)
    stress-ng: info:  [1496310] successful run completed in 10.04s
    ┌─(~)─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────(vasudeva.sk@chamunda:pts/8)─┐
    └─(17:08:08)──>                                                                                                                                                                                              3 ↵ ──(Mon,Jun19)─┘

As you can see I requested 50G memory to run stress-ng vm job but available
system memory including swap (zram) was 41G and Linux refused to allocate memory
and stress-ng could not proceed.

To make Linux kernel allocate memory in relaxed manner, as if there is infinite
amount of memory to be allocated we need to set sysctl value for
*vm.overcommit_memory* to *1*.

We can launch the above stress-ng with 50G now with this setting enabled but its
going to get OOM killed as really there is not that much memory on system :-)
